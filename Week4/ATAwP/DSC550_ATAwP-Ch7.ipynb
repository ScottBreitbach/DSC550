{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Context-Aware Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar-Based Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-Free Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NNP VP\n",
      "    VP -> V PP\n",
      "    PP -> P NP\n",
      "    NP -> DT N\n",
      "    NNP -> 'Gwen'\n",
      "    NNP -> 'George'\n",
      "    V -> 'looks'\n",
      "    V -> 'burns'\n",
      "    P -> 'in'\n",
      "    P -> 'for'\n",
      "    DT -> 'the'\n",
      "    N -> 'castle'\n",
      "    N -> 'ocean'\n",
      "S\n",
      "[S -> NNP VP, VP -> V PP, PP -> P NP, NP -> DT N, NNP -> 'Gwen', NNP -> 'George', V -> 'looks', V -> 'burns', P -> 'in', P -> 'for', DT -> 'the', N -> 'castle', N -> 'ocean']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# import nltk\n",
    "from nltk import CFG\n",
    "\n",
    "from reader import PickledCorpusReader\n",
    "\n",
    "GRAMMAR = \"\"\"\n",
    "    S -> NNP VP\n",
    "    VP -> V PP\n",
    "    PP -> P NP\n",
    "    NP -> DT N\n",
    "    NNP -> 'Gwen' | 'George'\n",
    "    V -> 'looks' | 'burns'\n",
    "    P -> 'in' | 'for'\n",
    "    DT -> 'the'\n",
    "    N -> 'castle' | 'ocean'\n",
    "    \"\"\"\n",
    "\n",
    "cfg = nltk.CFG.fromstring(GRAMMAR)\n",
    "\n",
    "print(cfg)\n",
    "print(cfg.start())\n",
    "print(cfg.productions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.regexp import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "chunker = RegexpParser(GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chunk.RegexpParser with 1 stages>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"\"\"\n",
    "    Dusty Baker proposed a simple solution to the Washington National's \n",
    "    early-season bullpen troubles Monday afternoon and it had nothing to \n",
    "    do with his maligned group of relievers.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our `KeyphraseExtractor` with a grammar and chunker to identify just the *noun phrases* using part-of-speech text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import category as unicat\n",
    "from itertools import groupby\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "GOODTAGS = frozenset(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])\n",
    "\n",
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wraps a PickledCorpusReader consisting of pos-tagged documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, grammar=GRAMMAR):\n",
    "        self.grammar = GRAMMAR\n",
    "        self.chunker = RegexpParser(self.grammar)\n",
    "    \n",
    "    def normalize(self, sent):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        is_punct = lambda word: all(unicat(char).startswith('P') for char in word)\n",
    "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
    "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
    "        return list(sent)\n",
    "    \n",
    "    def extract_keyphrases(self, document):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Yields extracted phrases.\n",
    "        \"\"\"\n",
    "        for sents in document:\n",
    "            for sent in sents:\n",
    "                sent = self.normalize(sent)\n",
    "                if not sent: continue\n",
    "                chunks = tree2conlltags(self.chunker.parse(sent))\n",
    "                phrases = [\n",
    "                    \" \".join(word for word, pos, chunk in group).lower()\n",
    "                    for key, group in groupby(\n",
    "                        chunks, lambda term: term[-1] != 'O'\n",
    "                    ) if key\n",
    "                ]\n",
    "                for phrase in phrases:\n",
    "                    yield phrase\n",
    "        \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield list(self.extract_keyphrases(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KeyphraseExtractor(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lonely city', 'heart piercing wisdom', 'loneliness', 'laing', 'everyone', 'feast later', 'point', 'own hermetic existence in new york', 'danger', 'thankfully', 'lonely city', 'cry for connection', 'overcrowded overstimulated world', 'blueprint of urban loneliness', 'emotion', 'calls', 'city', 'npr jason heller', 'olivia laing', 'lonely city', 'exploration of loneliness', 'others experiences in new york city', 'rumpus', 'review', 'lonely city', 'related posts']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from reader import PickledCorpusReader\n",
    "\n",
    "    corpus = PickledCorpusReader('../ATAwP/corpus')\n",
    "    docs = corpus.docs()\n",
    "\n",
    "    phrase_extractor = KeyphraseExtractor()\n",
    "    keyphrases = list(phrase_extractor.fit_transform(docs))\n",
    "    print(keyphrases[0])\n",
    "\n",
    "#     entity_extractor = EntityExtractor()\n",
    "#     entities = list(entity_extractor.fit_transform(docs))\n",
    "#     print(entities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "GOODLABELS = frozenset(['PERSON', 'ORGANIZATION', 'FACILITY', 'GPE', 'GSP'])\n",
    "\n",
    "class EntityExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, labels=GOODLABELS, **kwargs):\n",
    "        self.labels = labels\n",
    "\n",
    "    def get_entities(self, document):\n",
    "        entities = []\n",
    "        for paragraph in document:\n",
    "            for sentence in paragraph:\n",
    "                trees = ne_chunk(sentence)\n",
    "                for tree in trees:\n",
    "                    if hasattr(tree, 'label'):\n",
    "                        if tree.label() in self.labels:\n",
    "                            entities.append(\n",
    "                                ' '.join([child[0].lower() for child in tree])\n",
    "                                )\n",
    "        return entities\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.get_entities(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lonely city', 'loneliness', 'laing', 'new york', 'lonely city', 'npr', 'jason heller', 'olivia laing', 'lonely city', 'new york city', 'rumpus', 'lonely city', 'related']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from reader import PickledCorpusReader\n",
    "\n",
    "    corpus = PickledCorpusReader('../ATAwP/corpus')\n",
    "    docs = corpus.docs()\n",
    "\n",
    "#     phrase_extractor = KeyphraseExtractor()\n",
    "#     keyphrases = list(phrase_extractor.fit_transform(docs))\n",
    "#     print(keyphrases[0])\n",
    "\n",
    "    entity_extractor = EntityExtractor()\n",
    "    entities = list(entity_extractor.fit_transform(docs))\n",
    "    print(entities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
