{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gensim.models.ldamodel' has no attribute 'LdaTransformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ad03004a497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m'norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTextNormalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;34m'vect'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGensimTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         ('lda', ldamodel.LdaTransformer())])\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gensim.models.ldamodel' has no attribute 'LdaTransformer'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models import lsimodel, ldamodel\n",
    "# from gensim.sklearn_api import lsimodel, ldamodel\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [\n",
    "            self.normalize(document)\n",
    "            for document in documents\n",
    "        ]\n",
    "\n",
    "\n",
    "class GensimTfidfVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dirpath=\".\", tofull=False):\n",
    "        \"\"\"\n",
    "        Pass in a directory that holds the lexicon in corpus.dict and the\n",
    "        TFIDF model in tfidf.model (for now).\n",
    "\n",
    "        Set tofull = True if the next thing is a Scikit-Learn estimator\n",
    "        otherwise keep False if the next thing is a Gensim model.\n",
    "        \"\"\"\n",
    "        self._lexicon_path = os.path.join(dirpath, \"corpus.dict\")\n",
    "        self._tfidf_path = os.path.join(dirpath, \"tfidf.model\")\n",
    "\n",
    "        self.lexicon = None\n",
    "        self.tfidf = None\n",
    "        self.tofull = tofull\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        if os.path.exists(self._lexicon_path):\n",
    "            self.lexicon = Dictionary.load(self._lexicon_path)\n",
    "\n",
    "        if os.path.exists(self._tfidf_path):\n",
    "            self.tfidf = TfidfModel().load(self._tfidf_path)\n",
    "\n",
    "    def save(self):\n",
    "        self.lexicon.save(self._lexicon_path)\n",
    "        self.tfidf.save(self._tfidf_path)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        self.lexicon = Dictionary(documents)\n",
    "        self.tfidf = TfidfModel([self.lexicon.doc2bow(doc) for doc in documents], id2word=self.lexicon)\n",
    "        self.save()\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        def generator():\n",
    "            for document in documents:\n",
    "                vec = self.tfidf[self.lexicon.doc2bow(document)]\n",
    "                if self.tofull:\n",
    "                    yield sparse2full(vec)\n",
    "                else:\n",
    "                    yield vec\n",
    "        return list(generator())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from reader import PickledCorpusReader\n",
    "\n",
    "    corpus = PickledCorpusReader('../ATAwP/corpus')\n",
    "    docs = [\n",
    "        list(corpus.docs(fileids=fileid))[0]\n",
    "        for fileid in corpus.fileids()\n",
    "    ]\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('vect', GensimTfidfVectorizer()),\n",
    "        ('lda', ldamodel.LdaTransformer())])\n",
    "\n",
    "    model.fit_transform(docs)\n",
    "\n",
    "    print(model.named_steps['norm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
