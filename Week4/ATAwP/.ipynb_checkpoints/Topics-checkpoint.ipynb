{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "['make', '2016', 'january', 'use', 'spaghetti', 'would', 'pasta', 'recipe', 'pan', 'pie', '26', 'add', 'cheese', '2', 'think', 'like', 'time', 'look', 'well', '1', 'thanks', 'love', 'work']\n",
      "Topic #2:\n",
      "['tattoo', 'new', 'instagram', 'photo', 'post', 'see', 'show', 'ink', 'twitter', 'get', 'images', 'love', '2014', 'getty', 'caption', '2015', 'year', 'beckham', 'old', '2013', 'bieber', 'justin', 'singer']\n",
      "Topic #3:\n",
      "['1', '2', '3', '4', '0', '5', 'eng', '6', 'san', 'percent', 'hills', '8', 'thursday', 'p', '7', 'point', 'los', 'tuesday', '10', 'santa', 'division', 'diego', '9']\n",
      "Topic #4:\n",
      "['say', 'go', 'tell', 'call', 'city', 'news', 'report', 'could', 'think', 'know', 'group', 'statement', 'also', 'new', 'monday', 'official', 'wednesday', '000', 'comment', 'ask', 'help', 'include', 'want']\n",
      "Topic #5:\n",
      "['trump', 'donald', 'republican', 'campaign', 'candidate', 'presidential', 'news', 'gop', 'election', 'medium', 'make', 'cruz', 'hillary', 'would', 'clinton', 'policy', 'rally', 'real', 'general', 'people', 'ms', 'new', 'nominee']\n",
      "Topic #6:\n",
      "['â', 'loan', 'wells', 'fargo', 'fha', '\\x80\\x99', 'hud', 'mortgage', 'report', 'quality', 'insurance', 'lender', '\\x80\\x9c', 'claim', '\\x80\\x9d', 'pay', 'underwriting', 'program', 'finding', 'period', 'review', 'fargoâ', 'self']\n",
      "Topic #7:\n",
      "['june', 'may', 'minicamp', 'april', 'offseason', 'report', '26', 'mandatory', '9', 'workouts', '1', 'ota', '24', '31', '16', '14', '18', '6', '2', '7', '3', 'voluntary', 'program']\n",
      "Topic #8:\n",
      "['email', 'clinton', 'department', 'server', 'use', 'state', 'classified', 'official', 'government', 'security', 'blackberry', 'secretary', 'say', 'information', 'record', 'accord', 'could', 'account', 'private', 'system', 'personal', 'one', 'would']\n",
      "Topic #9:\n",
      "['$', 'million', 'billion', 'percent', '5', 'debt', 'wage', 'job', 'price', 'deal', 'raise', 'cost', 'pay', 'minimum', 'high', 'new', 'year', 'market', '15', 'revenue', '000', 'sale', 'per']\n",
      "Topic #10:\n",
      "['bergdahl', 'koenig', 'episode', 'would', 'taliban', 'say', 'one', 'tell', 'want', 'u', 'get', 'know', 'case', 'soldier', 'army', 'bowe', 'also', 'time', 'see', 'go', 'military', 'take', 'boal']\n",
      "Topic #11:\n",
      "['team', 'season', 'two', 'game', 'win', 'last', 'first', 'league', 'year', 'run', 'one', 'player', 'play', 'point', 'go', 'april', 'four', 'hit', 'lose', 'cubs', 'back', 'coach', 'month']\n",
      "Topic #12:\n",
      "['mr', 'say', 'rubio', 'rivera', 'ms', 'campaign', 'florida', 'house', 'miami', 'political', 'marco', 'state', 'two', 'president', 'silva', 'former', 'da', 'support', 'help', 'run', 'later', 'serve', 'friend']\n",
      "Topic #13:\n",
      "['nuclear', 'u', 'would', 'weapon', 'united', 'states', 'korea', 'north', 'ally', 'south', 'japan', 'proliferation', 'country', 'military', 'security', 'suggest', 'research', 'test', 'missile', 'policy', 'nonproliferation', 'world', 'threat']\n",
      "Topic #14:\n",
      "['game', 'make', 'play', 'player', 'work', 'new', 'daviau', 'like', 'complex', 'go', 'thing', 'launch', 'shadow', 'u', 'mobile', 'legacy', 'lot', 'donald', 'one', 'infinity', 'blade', 'time', 'design']\n",
      "Topic #15:\n",
      "['data', 'use', 'database', 'chart', 'art', 'information', 'science', '`', 'r', 'set', 'work', 'example', 'model', 'dplyr', 'create', 'step', 'bayesian', 'statistic', 'artist', 'need', 'algorithm', 'new', 'code']\n",
      "Topic #16:\n",
      "['tax', 'income', '£', 'pay', 'would', '000', 'return', 'rate', 'cameron', 'money', 'percent', 'year', 'gain', 'rich', 'plan', 'investment', 'make', 'raise', 'release', 'service', 'payment', 'capital', 'ira']\n",
      "Topic #17:\n",
      "['delegate', 'cruz', 'state', 'convention', 'kasich', 'vote', 'candidate', 'win', 'republican', 'trump', 'campaign', 'primary', 'gop', 'rubio', 'ted', 'nomination', 'rule', 'ballot', 'contest', 'race', 'john', 'ohio', 'nominee']\n",
      "Topic #18:\n",
      "['migrant', 'camp', '2016', 'calais', 'france', 'feb', 'refugee', 'jungle', 'french', 'photo', 'part', 'ap', 'getty', 'images', 'walk', 'court', 'half', 'makeshift', 'delay', 'accommodation', 'northern', 'authority', 'build']\n",
      "Topic #19:\n",
      "['school', 'student', 'high', 'district', 'university', 'education', 'college', 'parent', 'would', 'teacher', 'one', 'child', 'boy', 'black', 'year', 'public', 'white', 'cleveland', 'take', 'transgender', 'family', 'class', 'girl']\n",
      "Topic #20:\n",
      "['one', 'metric', 'user', 'story', 'use', 'see', 'way', 'reader', 'facebook', 'package', 'world', 'write', 'make', 'tell', 'u', 'travis', 'article', 'medium', 'page', 'also', 'problem', 'good', 'people']\n",
      "Topic #21:\n",
      "['get', 'like', 'go', 'think', 'know', 'want', 'make', 'thing', 'one', 'really', 'time', 'good', 'even', 'see', 'way', 'lot', 'back', 'come', 'something', 'much', 'could', 'take', 'never']\n",
      "Topic #22:\n",
      "['missile', 'russian', 'mh17', 'air', 'ukrainian', 'day', 'russia', 'plane', 'crash', 'theory', 'rebel', 'one', 'fighter', 'jet', 'buk', 'claim', 'shoot', 'launch', 'two', 'fire', 'see', 'army', 'show']\n",
      "Topic #23:\n",
      "['content', 'product', 'people', 'game', 'price', 'make', 'customer', 'sell', 'way', 'developer', 'support', 'set', 'work', 'video', 'marketplace', 'type', 'use', '$', 'see', 'create', 'creator', 'build', 'look']\n",
      "Topic #24:\n",
      "['salt', 'road', 'use', 'chloride', 'irondequoit', 'bay', 'creek', 'report', 'level', 'county', 'water', 'highway', 'high', 'stream', 'winter', 'time', 'see', 'local', 'concentration', 'one', 'monroe', 'study', 'kelly']\n",
      "Topic #25:\n",
      "['police', 'attack', 'abdeslam', 'brussels', 'paris', 'belgian', 'arrest', 'say', 'kill', 'raid', 'belgium', 'friday', 'people', 'suspect', 'authority', 'two', 'french', 'tell', 'terrorist', 'report', 'find', 'death', 'man']\n",
      "Topic #26:\n",
      "['thompson', 'spann', 'say', 'gray', 'former', 'campaign', 'prosecutor', 'office', 'go', 'u', 'attorney', 'question', 'interview', 'case', 'accord', 'witness', 'investigation', 'post', 'federal', 'would', 'time', 'c', 'mayor']\n",
      "Topic #27:\n",
      "['\\x80\\x99', 'â', 'sleep', '\\x80\\x94', 'dream', 'think', 'people', 'nightmare', 'thing', 'youâ', 'thatâ', '\\x80\\x9c', 'u', 'affect', 'wake', 'study', '\\x80\\x9d', 'iâ', 'process', 'dreaming', 'disturbed', 'experience', 'itâ']\n",
      "Topic #28:\n",
      "['business', 'bank', 'digital', 'new', 'customer', 'small', 'alternative', 'consumer', 'banking', 'lending', 'financial', 'market', 'work', 'one', 'team', 'service', 'need', 'industry', 'lender', 'big', 'use', 'allow', 'opportunity']\n",
      "Topic #29:\n",
      "['blazers', 'portland', 'trail', 'olshey', 'draft', 'lillard', 'pick', 'player', 'season', 'year', 'guy', 'team', 'like', 'go', 'game', 'would', 'could', 'point', 'mccollum', 'two', 'nba', 'summer', 'high']\n",
      "Topic #30:\n",
      "['oil', 'government', 'market', 'energy', 'economic', 'price', 'saudi', 'economy', 'country', 'big', 'world', 'year', 'arabia', 'sector', 'large', 'low', 'percent', 'fund', 'high', 'political', 'would', 'growth', 'financial']\n",
      "Topic #31:\n",
      "['mfa', 'writer', 'novel', 'program', 'like', 'percent', 'write', 'non', 'writing', 'way', 'literary', 'group', 'whether', 'university', 'word', 'creative', 'style', 'find', 'book', 'author', 'test', 'use', 'look']\n",
      "Topic #32:\n",
      "['de', 'en', 'la', 'que', 'el', 'los', 'se', 'delegados', 'una', 'un', 'del', 'con', 'al', 'cruz', 'sundance', 'su', 'peña', 'por', 'partido', 'image', 'directed', 'sousa', 'menos']\n",
      "Topic #33:\n",
      "['clinton', 'sanders', 'campaign', 'democratic', 'hillary', 'bernie', 'primary', 'voter', 'mrs', 'canova', 'big', 'michigan', 'president', 'would', 'race', 'vote', 'state', 'american', 'debate', 'think', 'new', 'presidential', 'bill']\n",
      "Topic #34:\n",
      "['cosby', 'case', 'attorney', 'woman', 'accuser', 'assault', 'sexual', 'lawsuit', 'hefner', 'one', 'make', 'sue', 'year', 'criminal', 'bill', 'allege', 'philadelphia', 'former', 'comedian', 'playboy', 'deposition', 'go', 'say']\n",
      "Topic #35:\n",
      "['isis', 'child', 'syria', 'kid', 'tell', 'group', 'raqqawi', 'caliphate', 'territory', 'also', 'propaganda', 'insider', 'iraq', 'video', 'business', 'like', 'young', 'military', 'age', 'force', 'control', 'curriculum', 'go']\n",
      "Topic #36:\n",
      "['show', 'episode', 'season', 'series', 'watch', 'miami', 'vice', 'time', 'star', 'first', 'one', 'even', 'new', 'two', 'take', 'crockett', 'like', 'television', 'best', 'life', 'also', 'tv', 'producer']\n",
      "Topic #37:\n",
      "['woman', 'men', 'life', 'one', 'girl', 'ms', 'boy', 'world', 'gender', 'fight', 'war', 'college', 'story', 'abortion', 'male', 'country', 'many', 'university', 'young', 'percent', 'female', 'british', 'old']\n",
      "Topic #38:\n",
      "['amc', '24', '20', '16', '30', 'mall', 'center', '18', '17', 'city', '14', 'new', 'prince', 'square', '22', 'singer', 'mills', 'west', 'film', '12', 'april', 'saturday', 'thursday']\n",
      "Topic #39:\n",
      "['obama', 'u', 'president', 'syria', 'plan', 'vietnam', 'former', 'cia', 'laux', 'syrian', 'united', 'official', 'would', 'states', 'force', 'administration', 'american', 'country', 'white', 'war', 'house', 'assad', 'military']\n",
      "Topic #40:\n",
      "['people', 'work', 'health', 'take', 'many', 'life', 'mental', 'help', 'find', 'time', 'job', 'pay', 'study', 'also', 'make', 'community', 'need', 'talk', 'country', 'someone', 'issue', 'day', 'care']\n",
      "Topic #41:\n",
      "['vr', 'vive', 'room', 'game', 'headset', 'make', 'scale', 'space', 'use', 'oculus', 'experience', 'virtual', 'time', 'htc', 'need', 'around', 'get', 'right', 'controller', 'go', 'launch', '3d', 'set']\n",
      "Topic #42:\n",
      "['brotherhood', 'organization', 'member', 'firewall', 'political', 'al', 'muslim', 'violent', 'islamist', 'egypt', 'qaeda', 'politics', 'leadership', 'terrorist', 'violence', 'extremism', 'might', 'organizational', 'would', 'longer', 'strategic', 'state', 'group']\n",
      "Topic #43:\n",
      "['home', 'house', 'city', 'one', 'live', 'housing', 'year', 'temporary', 'many', 'space', 'new', 'npr', 'kosuke', 'okahara', 'village', 'fukushima', 'also', 'property', 'old', 'rent', 'durham', 'make', 'neighborhood']\n",
      "Topic #44:\n",
      "['year', 'first', 'member', 'time', 'last', 'voting', 'old', 'vote', 'new', 'long', 'month', 'two', 'active', 'career', 'day', 'since', 'term', 'lose', 'branch', 'work', 'three', 'take', 'academy']\n",
      "Topic #45:\n",
      "['state', 'law', 'government', 'marijuana', 'court', 'federal', 'case', 'medical', 'would', 'use', 'drug', 'bill', 'right', 'justice', 'department', 'new', 'legal', 'one', 'allow', 'act', 'administration', 'also', 'congress']\n",
      "Topic #46:\n",
      "['china', 'chinese', 'military', 'u', 'base', 'sea', 'country', 'xi', 'year', 'report', 'power', 'wedding', 'south', 'one', 'beijing', 'new', 'pla', 'also', 'foreign', 'chinatown', 'percent', 'force', 'group']\n",
      "Topic #47:\n",
      "['company', 'product', 'brand', 'business', 'market', 'e', 'internet', 'year', 'lazada', 'service', 'google', 'uber', 'commerce', 'investor', 'southeast', 'million', 'like', 'apple', 'report', 'alibaba', 'asia', 'share', 'industry']\n",
      "Topic #48:\n",
      "['water', 'lead', 'california', 'high', 'level', 'test', 'food', 'school', 'state', 'find', 'flint', 'drought', 'also', 'drinking', 'groundwater', 'use', 'ppb', 'average', 'supply', 'drink', 'one', 'dry', 'surface']\n",
      "Topic #49:\n",
      "['party', 'republican', 'political', 'republicans', 'election', 'voter', 'house', 'candidate', 'one', 'president', 'vote', 'democrats', 'conservative', 'ryan', 'presidential', 'gop', 'leader', 'democratic', 'third', 'government', 'policy', 'system', 'support']\n",
      "Topic #50:\n",
      "['film', 'movie', 'work', 'make', 'one', 'world', 'also', 'project', 'come', 'like', 'play', 'character', 'look', 'find', 'director', 'love', 'story', 'star', 'big', 'shoot', 'something', 'always', 'audience']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from reader import PickledCorpusReader\n",
    "from transformers import TextNormalizer, GensimTfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# from gensim.sklearn_api import lsimodel, ldamodel\n",
    "from gensim.models import lsimodel, ldamodel\n",
    "\n",
    "def identity(words):\n",
    "    return words\n",
    "\n",
    "\n",
    "class SklearnTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA',\n",
    "        To use Non-Negative Matrix Factorization, set estimator to 'NMF',\n",
    "        otherwise, defaults to Latent Dirichlet Allocation ('LDA').\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
    "        elif estimator == 'NMF':\n",
    "            self.estimator = NMF(n_components=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = LatentDirichletAllocation(n_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('tfidf', CountVectorizer(tokenizer=identity,\n",
    "                                      preprocessor=None, lowercase=False)),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.model.fit_transform(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def get_topics(self, n=25):\n",
    "        \"\"\"\n",
    "        n is the number of top terms to show for each topic\n",
    "        \"\"\"\n",
    "        vectorizer = self.model.named_steps['tfidf']\n",
    "        model = self.model.steps[-1][1]\n",
    "        names = vectorizer.get_feature_names()\n",
    "        topics = dict()\n",
    "\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            features = topic.argsort()[:-(n - 1): -1]\n",
    "            tokens = [names[i] for i in features]\n",
    "            topics[idx] = tokens\n",
    "\n",
    "        return topics\n",
    "\n",
    "\n",
    "class GensimTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA'\n",
    "        otherwise defaults to Latent Dirichlet Allocation.\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = lsimodel.LsiTransformer(num_topics=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = ldamodel.LdaTransformer(num_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('vect', GensimTfidfVectorizer()),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "\n",
    "    def fit(self, documents):\n",
    "        self.model.fit(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = PickledCorpusReader('../ATAwP/corpus')\n",
    "\n",
    "    # With Sklearn\n",
    "    skmodel = SklearnTopicModels(estimator='NMF')\n",
    "    documents   = corpus.docs()\n",
    "\n",
    "    skmodel.fit_transform(documents)\n",
    "    topics = skmodel.get_topics()\n",
    "    for topic, terms in topics.items():\n",
    "        print(\"Topic #{}:\".format(topic+1))\n",
    "        print(terms)\n",
    "\n",
    "    # # With Gensim\n",
    "    # gmodel = GensimTopicModels(estimator='LSA')\n",
    "    #\n",
    "    # docs = [\n",
    "    #     list(corpus.docs(fileids=fileid))[0]\n",
    "    #     for fileid in corpus.fileids()\n",
    "    # ]\n",
    "    #\n",
    "    # gmodel.fit(docs)\n",
    "    #\n",
    "    # # retrieve the fitted lsa model from the named steps of the pipeline\n",
    "    # lsa = gmodel.model.named_steps['lsa'].gensim_model\n",
    "    #\n",
    "    # # show the topics with the token-weights for the top 10 most influential tokens:\n",
    "    # print(lsa.print_topics(10))\n",
    "\n",
    "\n",
    "    # # retrieve the fitted lda model from the named steps of the pipeline\n",
    "    # lda = gmodel.model.named_steps['lda'].gensim_model\n",
    "    #\n",
    "    # # show the topics with the token-weights for the top 10 most influential tokens:\n",
    "    # lda.print_topics(10)\n",
    "\n",
    "    # corpus = [\n",
    "    #     gmodel.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
    "    #     for doc in gmodel.model.named_steps['norm'].transform(docs)\n",
    "    # ]\n",
    "    #\n",
    "    #\n",
    "    # id2token = gmodel.model.named_steps['vect'].lexicon.id2token\n",
    "    #\n",
    "    # for word_id, freq in next(iter(corpus)):\n",
    "    #     print(id2token[word_id], freq)\n",
    "\n",
    "    # # get the highest weighted topic for each of the documents in the corpus\n",
    "    # def get_topics(vectorized_corpus, model):\n",
    "    #     from operator import itemgetter\n",
    "    #\n",
    "    #     topics = [\n",
    "    #         max(model[doc], key=itemgetter(1))[0]\n",
    "    #         for doc in vectorized_corpus\n",
    "    #     ]\n",
    "    #\n",
    "    #     return topics\n",
    "    #\n",
    "    # topics = get_topics(corpus,lda)\n",
    "    #\n",
    "    # for topic, doc in zip(topics, docs):\n",
    "    #     print(\"Topic:{}\".format(topic))\n",
    "    #     print(doc)\n",
    "    #\n",
    "    ## retreive the fitted vectorizer or the lexicon if needed\n",
    "    # tfidf = gmodel.model.named_steps['vect'].tfidf\n",
    "    # lexicon = gmodel.model.named_steps['vect'].lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
