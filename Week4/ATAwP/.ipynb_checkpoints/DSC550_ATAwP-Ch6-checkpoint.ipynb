{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Clustering for Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering by Document Similarity\n",
    "### Partitive Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means clustering\n",
    "Initialize the NLTK `KMeansClusterer` with our desired number of clusters (*k*) and our preferred distance measure (`cosine_distance`), and avoid a result iwth clusters that contain no documents.  \n",
    "Then we'll add our no-op `fit()` method and a `transform()` method that calls the internal `KMeansClusterer` model's `cluster()` method, specifying that each document should be assigned a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class KMeansClusters(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, k=7):\n",
    "        '''\n",
    "        k is the number of clusters\n",
    "        model is the implementation of Kmeans\n",
    "        '''\n",
    "        self.k = k\n",
    "        self.distance = nltk.cluster.util.cosine_distance\n",
    "        self.model = KMeansClusterer(self.k, self.distance, \n",
    "                                     avoid_empty_clusters=True)\n",
    "    \n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        '''\n",
    "        Fits the K-Means model to one-hot vectorized documents\n",
    "        '''\n",
    "        return self.model.cluster(documents, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and vectorize documents for our `KMeansClusters` class.  \n",
    "Instead of returning a representation of documents as bags-of-words, this version of the `TextNormailzer` will perfrom stopwords removal and lemmatization and return a string for each document.  \n",
    "*(Note: most of this is from Ch4, p72)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [\n",
    "            ' '.join(self.normalize(document))\n",
    "            for document in documents\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize documents after normalization (before clustering) with `OneHotVectorizer` class.  \n",
    "Use Scikit-Learn's `CountVectorizer` with `binary=True`, which will wrap both frequency encoding and binarization.  \n",
    "The `transform()` method will return a representation of each doucment as a one-hot vectorized array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class OneHotVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(binary=True)\n",
    "        \n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        freqs = self.vectorizer.fit_transform(documets)\n",
    "        return [freq.toarray()[0] for freq in freqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a `Pipeline` inside our `main()` execution to perform *k*-means clustering.  \n",
    "Initialize a `PickledCorpusReader` (Ch3, p51), specifying use of only the \"news\" category.  \n",
    "Initialize a pipeline to streamline our custom `TextNormalizer`, `OneHotVectorizer`, and `KMeansClusters` classes. By calling `fit_transform()` on the pipeline, we perfrom each of the steps in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c0b613e97ba4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mdocuments\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[0mskmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-c0b613e97ba4>\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m    366\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1130\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from reader import PickledCorpusReader\n",
    "from transformers import TextNormalizer, GensimTfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# from gensim.sklearn_api import lsimodel, ldamodel\n",
    "from gensim.models import lsimodel, ldamodel\n",
    "\n",
    "def identity(words):\n",
    "    return words\n",
    "\n",
    "\n",
    "class SklearnTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA',\n",
    "        To use Non-Negative Matrix Factorization, set estimator to 'NMF',\n",
    "        otherwise, defaults to Latent Dirichlet Allocation ('LDA').\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
    "        elif estimator == 'NMF':\n",
    "            self.estimator = NMF(n_components=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = LatentDirichletAllocation(n_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('tfidf', CountVectorizer(tokenizer=identity,\n",
    "                                      preprocessor=None, lowercase=False)),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.model.fit_transform(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def get_topics(self, n=25):\n",
    "        \"\"\"\n",
    "        n is the number of top terms to show for each topic\n",
    "        \"\"\"\n",
    "        vectorizer = self.model.named_steps['tfidf']\n",
    "        model = self.model.steps[-1][1]\n",
    "        names = vectorizer.get_feature_names()\n",
    "        topics = dict()\n",
    "\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            features = topic.argsort()[:-(n - 1): -1]\n",
    "            tokens = [names[i] for i in features]\n",
    "            topics[idx] = tokens\n",
    "\n",
    "        return topics\n",
    "\n",
    "\n",
    "class GensimTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA'\n",
    "        otherwise defaults to Latent Dirichlet Allocation.\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = lsimodel.LsiTransformer(num_topics=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = ldamodel.LdaTransformer(num_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('vect', GensimTfidfVectorizer()),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "\n",
    "    def fit(self, documents):\n",
    "        self.model.fit(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = PickledCorpusReader('../corpus')\n",
    "\n",
    "    # With Sklearn\n",
    "    skmodel = SklearnTopicModels(estimator='NMF')\n",
    "    documents   = corpus.docs()\n",
    "\n",
    "    skmodel.fit_transform(documents)\n",
    "    topics = skmodel.get_topics()\n",
    "    for topic, terms in topics.items():\n",
    "        print(\"Topic #{}:\".format(topic+1))\n",
    "        print(terms)\n",
    "\n",
    "    # # With Gensim\n",
    "    # gmodel = GensimTopicModels(estimator='LSA')\n",
    "    #\n",
    "    # docs = [\n",
    "    #     list(corpus.docs(fileids=fileid))[0]\n",
    "    #     for fileid in corpus.fileids()\n",
    "    # ]\n",
    "    #\n",
    "    # gmodel.fit(docs)\n",
    "    #\n",
    "    # # retrieve the fitted lsa model from the named steps of the pipeline\n",
    "    # lsa = gmodel.model.named_steps['lsa'].gensim_model\n",
    "    #\n",
    "    # # show the topics with the token-weights for the top 10 most influential tokens:\n",
    "    # print(lsa.print_topics(10))\n",
    "\n",
    "\n",
    "    # # retrieve the fitted lda model from the named steps of the pipeline\n",
    "    # lda = gmodel.model.named_steps['lda'].gensim_model\n",
    "    #\n",
    "    # # show the topics with the token-weights for the top 10 most influential tokens:\n",
    "    # lda.print_topics(10)\n",
    "\n",
    "    # corpus = [\n",
    "    #     gmodel.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
    "    #     for doc in gmodel.model.named_steps['norm'].transform(docs)\n",
    "    # ]\n",
    "    #\n",
    "    #\n",
    "    # id2token = gmodel.model.named_steps['vect'].lexicon.id2token\n",
    "    #\n",
    "    # for word_id, freq in next(iter(corpus)):\n",
    "    #     print(id2token[word_id], freq)\n",
    "\n",
    "    # # get the highest weighted topic for each of the documents in the corpus\n",
    "    # def get_topics(vectorized_corpus, model):\n",
    "    #     from operator import itemgetter\n",
    "    #\n",
    "    #     topics = [\n",
    "    #         max(model[doc], key=itemgetter(1))[0]\n",
    "    #         for doc in vectorized_corpus\n",
    "    #     ]\n",
    "    #\n",
    "    #     return topics\n",
    "    #\n",
    "    # topics = get_topics(corpus,lda)\n",
    "    #\n",
    "    # for topic, doc in zip(topics, docs):\n",
    "    #     print(\"Topic:{}\".format(topic))\n",
    "    #     print(doc)\n",
    "    #\n",
    "    ## retreive the fitted vectorizer or the lexicon if needed\n",
    "    # tfidf = gmodel.model.named_steps['vect'].tfidf\n",
    "    # lexicon = gmodel.model.named_steps['vect'].lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# */site-packages is where your current session is running its python out of\n",
    "site_path = ''\n",
    "for path in sys.path:\n",
    "    if 'site-packages' in path.split('/')[-1]:\n",
    "        print(path)\n",
    "        site_path = path\n",
    "# search to see if gensim in installed packages\n",
    "if len(site_path) > 0:\n",
    "    if not 'gensim' in os.listdir(site_path):\n",
    "        print('package not found')\n",
    "    else:\n",
    "        print('gensim installed')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !py -0p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# py -3.9 -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.sklearn_integration'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1e57b6935dd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn_integration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsimodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mldamodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.sklearn_integration'"
     ]
    }
   ],
   "source": [
    "from gensim.sklearn_integration import lsimodel, ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import lsimodel, ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
