{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08711c1a-6465-4272-8b65-c9ad29c58696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae1dbbb4-0a24-4b0c-af6d-b8fe8671a8a8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff8781e-dc9a-49ed-921c-46790a044449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn-intelex\n",
    "# conda install scikit-learn-intelex -c intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d5323-591c-4095-90cd-451bed9efa43",
   "metadata": {},
   "source": [
    "Intel(R) Extension for Scikit-learn:  \n",
    "https://intel.github.io/scikit-learn-intelex/get_started.html  \n",
    "https://pypi.org/project/scikit-learn-intelex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f3f7ab-b6a8-48c0-a7fd-44c168be606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Use all processor cores\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c95045-d93a-402c-b0ef-7dd4f30362d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import jsonlines\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f54b7b-cb98-4d2f-80ce-d43b42efbcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load JSON data into a list of dictionaries\n",
    "data = []\n",
    "with jsonlines.open('categorized-comments.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06f2fee-6187-4c41-aa46-c895a2e1d6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to DataFrame\n",
    "cat_comments_df = pd.DataFrame(data)\n",
    "cat_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "672091bd-5c4e-44ac-bf07-083240fe009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories are:\n",
      " - sports\n",
      " - science_and_technology\n",
      " - video_games\n"
     ]
    }
   ],
   "source": [
    "# Check out the categories\n",
    "categories = cat_comments_df.cat.unique()\n",
    "print(\"The categories are:\")\n",
    "for category in categories:\n",
    "    print(\" -\", category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5fe023-11a7-40e6-b58d-c23698ec1767",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916dc773-8ab3-40db-a175-1531a3349d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e49e36-f7e8-4f29-b0a7-4a737b5fa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cat_comments_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e771af6-1156-494a-98ca-cc9d88ca9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0526f61-6b97-42fd-a553-75ccb533252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_dict = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "def cleanText(string):\n",
    "    '''Processes string and returns cleaned up list of words'''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    string = string.translate(punctuation_dict)\n",
    "    \n",
    "    # Remove newlines\n",
    "    string = string.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Remove stopwords\n",
    "    string = [word for word in string.split() if word not in stopwords_dict]\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2031e4dc-12ea-4d63-a71e-8b76a1f5ea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean up the text in the 'txt' column\n",
    "df.txt = df.txt.apply(lambda string: cleanText(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd94e7ae-8e42-48f0-9134-91cb123e3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8ff1a4-fba0-41ea-8f00-d6dd2e9ef8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "df['txt_stems'] = df.txt.apply(lambda words: [porter.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08c882df-3b49-4e1d-8e0b-f4a588fe2430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "      <th>txt_stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[barely, better, gabbert, significantly, bette...</td>\n",
       "      <td>[bare, better, gabbert, significantli, better,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[fuck, ducks, angels, welcome, new, niners, fans]</td>\n",
       "      <td>[fuck, duck, angel, welcom, new, niner, fan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[drafted, wrs, matt, millen, probably]</td>\n",
       "      <td>[draft, wr, matt, millen, probabl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[done]</td>\n",
       "      <td>[done]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[noo]</td>\n",
       "      <td>[noo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt  \\\n",
       "0  sports  [barely, better, gabbert, significantly, bette...   \n",
       "1  sports  [fuck, ducks, angels, welcome, new, niners, fans]   \n",
       "2  sports             [drafted, wrs, matt, millen, probably]   \n",
       "3  sports                                             [done]   \n",
       "4  sports                                              [noo]   \n",
       "\n",
       "                                           txt_stems  \n",
       "0  [bare, better, gabbert, significantli, better,...  \n",
       "1       [fuck, duck, angel, welcom, new, niner, fan]  \n",
       "2                 [draft, wr, matt, millen, probabl]  \n",
       "3                                             [done]  \n",
       "4                                              [noo]  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64d717-2cad-4006-bee3-f2168d0daedf",
   "metadata": {},
   "source": [
    "## Prepare Text for Model-Building\n",
    "### **--COMMENT ALL THIS CODE--**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bd63695-3fc8-447b-a594-87eff3caab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48592409-75b7-4351-8487-f2dd5b5b323f",
   "metadata": {},
   "source": [
    "### Back up, let's sample to equal sized groups:\n",
    "https://stackoverflow.com/questions/41345289/getting-a-random-sample-in-python-dataframe-by-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7e3035-2856-40e9-9f77-f713096ab1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_group = df.groupby('cat', as_index=False, group_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce00fd82-b94c-4147-a2e0-ae2493f93d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "balancedDF = cat_group.apply(lambda s: s.sample(25000, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8351d236-f668-4bc7-a418-1fb5e194d440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports                    25000\n",
       "video_games               25000\n",
       "science_and_technology    25000\n",
       "Name: cat, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balancedDF.cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4387b-dd6f-4288-b568-d6e41361c345",
   "metadata": {},
   "source": [
    "### Convert to a word-count vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c31847-1ea9-4a77-b955-ffd4d5c0fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "698e10d4-c9f9-4d5e-8433-ae6461638de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data, string = [], \" \"\n",
    "\n",
    "for text in balancedDF.txt_stems:\n",
    "    text_data.append(string.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea4b31d-f87f-4daa-b0cd-ed3f0f37eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<75000x41427 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 961832 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Word-count vector as a sparse matrix\n",
    "bal_sparseWCV = count.fit_transform(text_data)\n",
    "bal_sparseWCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9703974-69e0-4697-b38f-47f6ccf1aa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 41427)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bal_sparseWCV.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b25f73-b1c9-4ade-a7f1-ad2bfddde9da",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ba88cc7-04da-4ab1-ba86-cef36a896d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and labels\n",
    "X = bal_sparseWCV\n",
    "y = balancedDF.cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30008fb8-7bd4-449a-b995-c3ed5b62330c",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af7fe969-8c64-4e2a-b6e1-a8626a938951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) # Default is 1/4 --> test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c60bcc-97f1-4163-a64b-46bcb7e827d5",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a71c72-a8b1-410b-bb4e-2293ff1c3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e352696-f476-4fb1-aa5f-3d3de31cde41",
   "metadata": {},
   "source": [
    "## First let's optimize some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "052965f9-3b20-40b8-934f-487976fdf561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11h 28min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'activation': ['relu'], 'alpha': [0.0001],\n",
       "                         'hidden_layer_sizes': [(30,), (50, 30), (100,)],\n",
       "                         'learning_rate': ['constant'], 'solver': ['adam']})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mlp_gs = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(30,), (50, 30), (100,)], #[(20,20, 20),(30,),(40,)],\n",
    "    'activation': ['relu'], #['tanh', 'relu'], \n",
    "    'solver': ['adam'],#, 'sgd'], \n",
    "    'alpha': [0.0001],#, 0.05],\n",
    "    'learning_rate': ['constant'],#,'adaptive'], # only used when solver is 'sgd'\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(X, y) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6be6d40b-0e78-424d-83e3-5c536d24a1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8a2fe62-cc7b-4525-bc54-b23974a908d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the test set:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.93      0.96      0.95      6248\n",
      "                sports       0.92      0.96      0.94      6255\n",
      "           video_games       0.99      0.92      0.95      6247\n",
      "\n",
      "              accuracy                           0.95     18750\n",
      "             macro avg       0.95      0.95      0.95     18750\n",
      "          weighted avg       0.95      0.95      0.95     18750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test , clf.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9df2fd4-9636-4abb-852a-d1f4fde279c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (30,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa845f1e-6148-4ff1-a2b5-2ba7cd01e31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the test set:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.89      0.90      0.90      6248\n",
      "                sports       0.89      0.87      0.88      6255\n",
      "           video_games       0.86      0.87      0.87      6247\n",
      "\n",
      "              accuracy                           0.88     18750\n",
      "             macro avg       0.88      0.88      0.88     18750\n",
      "          weighted avg       0.88      0.88      0.88     18750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test , clf.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b2507-38b6-4a79-999b-a1eacbb7f886",
   "metadata": {},
   "source": [
    "#### Create an instance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05085d51-00de-45a1-a548-4cd6b98db7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_mlp = MLPClassifier(max_iter=100, hidden_layer_sizes=(30,), activation='relu', \n",
    "                        solver='adam', alpha=0.0001, learning_rate='constant',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb4294b0-83ed-45e4-85c4-63d0c9dcdd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a870dc64-8f17-4165-8b75-f189ba753823",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_mlp = MLPClassifier(max_iter=100, hidden_layer_sizes=(100,), activation='relu', \n",
    "                        solver='adam', alpha=0.0001, learning_rate='constant',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b4351-20bb-4d36-99f8-363ef3408446",
   "metadata": {},
   "source": [
    "#### Fit the training data to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0058de3d-bb1b-4f53-8418-9d071d672419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 26min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=100)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bal_mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aceb67-db79-46a4-aa03-06c63e34debb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3da69c40-ab3e-4d4a-b35d-562455146e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from win32com.client import Dispatch\n",
    "speak = Dispatch(\"SAPI.SpVoice\").Speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db4dce9d-17a9-49ad-92eb-937af8e293c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speak(\"modeling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f14850-e95a-4fb9-aa7e-25ff50764559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77e51a1d-7380-4d77-90e7-747cd97eacce",
   "metadata": {},
   "source": [
    "### Predictions / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4bac9f05-5b07-4d76-bf53-fda82be75c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 82.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get predictions\n",
    "predictions = bal_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9cdf60-1dbe-40be-be90-a30978d754df",
   "metadata": {},
   "source": [
    "### Evaluate how well our model performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efc7be1d-c6f7-41da-b696-26da8cb43fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840dee9-7ec8-491b-81b4-66d3ded17667",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab4ce272-5f3a-4dcc-8de9-9c4a7ba59e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4978  560  710]\n",
      " [ 767 4402 1086]\n",
      " [1112 1179 3956]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b760f81-ce8b-4126-b6db-3616174bded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4592  883  773]\n",
      " [ 439 4727 1089]\n",
      " [ 673 1478 4096]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac596ea-2da7-4301-9da1-88080e00c800",
   "metadata": {},
   "source": [
    "#### Precision / Recall / F1 / Suport (Classification report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7406752-af0f-4a9a-8932-cc1a6a290e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.73      0.80      0.76      6248\n",
      "                sports       0.72      0.70      0.71      6255\n",
      "           video_games       0.69      0.63      0.66      6247\n",
      "\n",
      "              accuracy                           0.71     18750\n",
      "             macro avg       0.71      0.71      0.71     18750\n",
      "          weighted avg       0.71      0.71      0.71     18750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82265954-60ba-4436-a323-04fe87959bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.81      0.73      0.77      6248\n",
      "                sports       0.67      0.76      0.71      6255\n",
      "           video_games       0.69      0.66      0.67      6247\n",
      "\n",
      "              accuracy                           0.72     18750\n",
      "             macro avg       0.72      0.72      0.72     18750\n",
      "          weighted avg       0.72      0.72      0.72     18750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78f9b0-43bf-4382-88d8-9543d1ec8105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21922e44-d730-461c-9ba2-79d3766633d0",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f49498b4-1203-4764-aa11-9925fe4eddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Save a model to disk using pickle\n",
    "import pickle\n",
    "\n",
    "model = bal_mlp\n",
    "pkl_filename = 'mlp_bal_spWCV_optimized.pkl'\n",
    "\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c94d5c-a18b-4412-ad88-a6f35baebc14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
