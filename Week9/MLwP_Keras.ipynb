{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea25ea-e6f0-4953-8d7c-4b1173cef176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f30b8d64-8da6-4f6a-b2d0-469719770dd5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e6883a-36fc-4a0b-9a85-ff9dbc428c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Use all processor cores\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7118546c-596e-4e8b-8441-dc4f85dbb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import jsonlines\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f27f71-9230-4c96-a279-e8c59060af1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load JSON data into a list of dictionaries\n",
    "data = []\n",
    "with jsonlines.open('categorized-comments.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31e027c-9035-4180-9463-1e173b726dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to DataFrame\n",
    "cat_comments_df = pd.DataFrame(data)\n",
    "cat_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92bbc2a-74d5-44bd-8baf-5c983f76fef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories are:\n",
      " - sports\n",
      " - science_and_technology\n",
      " - video_games\n"
     ]
    }
   ],
   "source": [
    "# Check out the categories\n",
    "categories = cat_comments_df.cat.unique()\n",
    "print(\"The categories are:\")\n",
    "for category in categories:\n",
    "    print(\" -\", category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdec428-6518-49d7-a835-34bcdeb1a17b",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce34658-989c-4470-b8e8-3bd056adf4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef1ee08-fd03-40ce-b050-28baae4567c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cat_comments_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3594a201-4a5b-4f3b-9319-7982ccc57140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d96c8c-395f-488d-8dab-bd01f2b8d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_dict = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "def cleanText(string):\n",
    "    '''Processes string and returns cleaned up list of words'''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    string = string.translate(punctuation_dict)\n",
    "    \n",
    "    # Remove newlines\n",
    "    string = string.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Remove stopwords\n",
    "    string = [word for word in string.split() if word not in stopwords_dict]\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8766623c-9b18-4ff8-9f1d-ce1a3d158727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean up the text in the 'txt' column\n",
    "df.txt = df.txt.apply(lambda string: cleanText(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79207aa9-2935-4d98-8b0d-8eb74b55aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31485c56-c9df-4819-a3ad-2dc7e3459f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "df['txt_stems'] = df.txt.apply(lambda words: [porter.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3adb353-b241-4daa-b3cb-7697ef564a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['txt_str'] = df.txt_stems.apply(lambda s: ' '.join(map(str, s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f7b367-dcea-467b-8f6e-adc148661cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "      <th>txt_stems</th>\n",
       "      <th>txt_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[barely, better, gabbert, significantly, bette...</td>\n",
       "      <td>[bare, better, gabbert, significantli, better,...</td>\n",
       "      <td>bare better gabbert significantli better year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[fuck, ducks, angels, welcome, new, niners, fans]</td>\n",
       "      <td>[fuck, duck, angel, welcom, new, niner, fan]</td>\n",
       "      <td>fuck duck angel welcom new niner fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[drafted, wrs, matt, millen, probably]</td>\n",
       "      <td>[draft, wr, matt, millen, probabl]</td>\n",
       "      <td>draft wr matt millen probabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[done]</td>\n",
       "      <td>[done]</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[noo]</td>\n",
       "      <td>[noo]</td>\n",
       "      <td>noo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt  \\\n",
       "0  sports  [barely, better, gabbert, significantly, bette...   \n",
       "1  sports  [fuck, ducks, angels, welcome, new, niners, fans]   \n",
       "2  sports             [drafted, wrs, matt, millen, probably]   \n",
       "3  sports                                             [done]   \n",
       "4  sports                                              [noo]   \n",
       "\n",
       "                                           txt_stems  \\\n",
       "0  [bare, better, gabbert, significantli, better,...   \n",
       "1       [fuck, duck, angel, welcom, new, niner, fan]   \n",
       "2                 [draft, wr, matt, millen, probabl]   \n",
       "3                                             [done]   \n",
       "4                                              [noo]   \n",
       "\n",
       "                                             txt_str  \n",
       "0  bare better gabbert significantli better year ...  \n",
       "1               fuck duck angel welcom new niner fan  \n",
       "2                       draft wr matt millen probabl  \n",
       "3                                               done  \n",
       "4                                                noo  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d46ff-8fdb-4286-b331-59a67d17ee36",
   "metadata": {},
   "source": [
    "## Prepare Text for Model-Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b475336-a66f-44c9-8109-1815a3528046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9d8fa-a6ed-42e0-a87e-be8e8e3e4210",
   "metadata": {},
   "source": [
    "### Back up, let's sample to equal sized groups:\n",
    "https://stackoverflow.com/questions/41345289/getting-a-random-sample-in-python-dataframe-by-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "befe048d-7e6b-4141-98ea-3fc1a7838d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_group = df.groupby('cat', as_index=False, group_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f17472-58ef-417a-ba37-a29a4b24bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "balancedDF = cat_group.apply(lambda s: s.sample(25000, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59945f35-784e-460d-91ae-9537486e9731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports                    25000\n",
       "video_games               25000\n",
       "science_and_technology    25000\n",
       "Name: cat, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balancedDF.cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669fa575-0d37-4ae8-aa04-946a1776657e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "      <th>txt_stems</th>\n",
       "      <th>txt_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9949</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>[shit, 99, probably, never, written, lick, cod...</td>\n",
       "      <td>[shit, 99, probabl, never, written, lick, code...</td>\n",
       "      <td>shit 99 probabl never written lick code either...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>[qualcomm]</td>\n",
       "      <td>[qualcomm]</td>\n",
       "      <td>qualcomm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18759</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>[rule, 4]</td>\n",
       "      <td>[rule, 4]</td>\n",
       "      <td>rule 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>[iphones, copied, htc, way, around]</td>\n",
       "      <td>[iphon, copi, htc, way, around]</td>\n",
       "      <td>iphon copi htc way around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22746</th>\n",
       "      <td>science_and_technology</td>\n",
       "      <td>[biggest, gripe, cant, customize, skip, interv...</td>\n",
       "      <td>[biggest, gripe, cant, custom, skip, interv, p...</td>\n",
       "      <td>biggest gripe cant custom skip interv podcast ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          cat  \\\n",
       "9949   science_and_technology   \n",
       "7800   science_and_technology   \n",
       "18759  science_and_technology   \n",
       "3583   science_and_technology   \n",
       "22746  science_and_technology   \n",
       "\n",
       "                                                     txt  \\\n",
       "9949   [shit, 99, probably, never, written, lick, cod...   \n",
       "7800                                          [qualcomm]   \n",
       "18759                                          [rule, 4]   \n",
       "3583                 [iphones, copied, htc, way, around]   \n",
       "22746  [biggest, gripe, cant, customize, skip, interv...   \n",
       "\n",
       "                                               txt_stems  \\\n",
       "9949   [shit, 99, probabl, never, written, lick, code...   \n",
       "7800                                          [qualcomm]   \n",
       "18759                                          [rule, 4]   \n",
       "3583                     [iphon, copi, htc, way, around]   \n",
       "22746  [biggest, gripe, cant, custom, skip, interv, p...   \n",
       "\n",
       "                                                 txt_str  \n",
       "9949   shit 99 probabl never written lick code either...  \n",
       "7800                                            qualcomm  \n",
       "18759                                             rule 4  \n",
       "3583                           iphon copi htc way around  \n",
       "22746  biggest gripe cant custom skip interv podcast ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balancedDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4498bb4-9c22-4964-ae89-c3de50a745fc",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3757c506-6656-4411-9189-a8d226eb0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and labels\n",
    "X = balancedDF.txt_str\n",
    "y = balancedDF.cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91a6df-1851-4521-9e37-164dcb33d606",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3c9d0ec-c192-4394-a5c2-a32648e5d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81944ae8-3f7e-481b-bc49-4b63d7193fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) # Default is 1/4 --> test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc05530-340f-4219-af2b-f8cb1cf6e5f5",
   "metadata": {},
   "source": [
    "## 2. Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdedc4ee-7a71-48ad-97db-dd6daabfe237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb2c937e-777e-4fb2-9e38-a46136821a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=1, activation='relu')) # \n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb92609-c3cd-4d6c-ad0f-d2f925a8a8aa",
   "metadata": {},
   "source": [
    "## 3. Compile Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9f5cf36-6611-488e-930d-e05abbf9ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "479965b1-17ba-4b8d-89cf-1a40830e9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #  optimizer=RMSprop(lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e309f-af8c-479b-a59e-4697159a3b81",
   "metadata": {},
   "source": [
    "## 4. Fit Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fe010e2-29f5-4cc5-920a-f683bd378c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # fit the keras model on the dataset\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731d6b4-d6cc-4327-8b5d-5f269db4e653",
   "metadata": {},
   "source": [
    "# and now for something completely different..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76153cc-9358-4ce8-9731-5f55573c02ae",
   "metadata": {},
   "source": [
    "## 20.4 Training a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12535ab5-ce44-4af2-a87d-41d358c2309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ff50728-19be-4a4b-b956-e0e69f1aef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set random seed\n",
    "# np.random.seed(0)\n",
    "\n",
    "# # Set the number of features we want\n",
    "number_of_features = 5000\n",
    "\n",
    "# # Load feature and target data\n",
    "# # data = reuters.load_data(num_words=number_of_features)\n",
    "# data = balancedDF.copy()\n",
    "# X = data.txt_str\n",
    "# y = data.cat\n",
    "# # (data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_vector_train, target_vector_test = train_test_split(X, y) # Default is 1/4 --> test\n",
    "\n",
    "# # Convert feature data to a one-hot encoded feature matrix\n",
    "# tokenizer = Tokenizer(num_words=number_of_features)\n",
    "# features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "# features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# One-hot encode target vector to create a target matrix\n",
    "# target_train = to_categorical(target_vector_train)\n",
    "# target_test = to_categorical(target_vector_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b148eb7-b9e7-4d9c-bfbf-13e4c75df291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bcf7475-1208-4292-b800-3773b5e0564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data, string = [], \" \"\n",
    "\n",
    "for text in balancedDF.txt_stems:\n",
    "    text_data.append(string.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d246dd3-2cca-4f0b-b4ca-0e9b65c7a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3502d002-6056-4d8f-9116-a82aca3df637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_train = tokenizer.texts_to_matrix(data_train, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ac2c0e3-48b2-41aa-bf2d-503f791daa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "features_test = tokenizer.texts_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "851fe865-7791-44a3-b714-84fe8464db0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_train' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target\n",
    "                      epochs=3, # Three epochs\n",
    "                      verbose=0, # No output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59a77025-851a-4ce8-b942-ac22319ba58c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-62f4e07eed62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# View target matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtarget_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target_train' is not defined"
     ]
    }
   ],
   "source": [
    "# View target matrix\n",
    "target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30f096c6-e62b-4801-ac42-315a02200378",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-978fae38ff7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target_train' is not defined"
     ]
    }
   ],
   "source": [
    "target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4081d-ccfd-4a1b-991e-221c4cd13d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc1bf6-9258-4af9-83e9-e8ab0841cd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85552069-d96b-440f-8b43-a0c5d85ad088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa151370-309f-455e-9101-5da510003617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc894abf-de9b-4a70-b8fa-659ddea3dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe0790-854c-423f-ae75-7c40ef7e66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from win32com.client import Dispatch\n",
    "speak = Dispatch(\"SAPI.SpVoice\").Speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340a7ae-bc82-4226-ba70-8c6640eb86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak(\"modeling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f58517-87b6-42d8-b9e4-e0a8816d6c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033ccdd-1ca6-4c12-86ee-2dbde30103a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3687b3e-800d-4b37-beae-b2c8d30f20e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12a4f0a2-0397-41d3-a044-13c9443c9ff3",
   "metadata": {},
   "source": [
    "## 20.4 Training a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70b8bf7f-2e1a-458f-8722-adf76056a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22dceb43-6ce8-444b-ac28-6ae0f8d63d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 5000\n",
    "\n",
    "# Load feature and target data\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "\n",
    "# Convert feature data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# One-hot encode target vector to create a target matrix\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target\n",
    "                      epochs=3, # Three epochs\n",
    "                      verbose=0, # No output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "287d64e2-f15b-4e21-bf6a-0354e663df57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2bdcf0a-918a-4a0b-8811-51e144bb825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),\n",
       "       list([1, 3267, 699, 3434, 2295, 56, 2, 2, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]),\n",
       "       list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12]),\n",
       "       ...,\n",
       "       list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 2, 2, 2, 699, 2, 2, 2, 699, 244, 2, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 2, 7, 4, 2, 654, 5, 2, 2, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12]),\n",
       "       list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 2, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 2, 43, 359, 5, 4, 326, 753, 364, 17, 12]),\n",
       "       list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 2, 2, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 2, 1688, 340, 7, 194, 2, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 2, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 2, 2, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 2, 114, 2, 1752, 7, 4, 113, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f3cfa-33e2-4a2f-999e-3d09f8da20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966247c-2928-43ca-8a8d-eb916cddf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3e158-6b1e-4523-af9f-a13ae40b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aedd1d-777f-4ac6-8381-7e1888097331",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06ef8b-30f2-40c1-8824-04d20ca584b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text\n",
    "text_data = np.array(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af47c92-172a-4305-b9e0-6f504937b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac98e23-75c1-40ea-9f70-e7e29893cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467be60e-3e0c-465e-a692-1fa75f7fd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d3d34-44b5-44a0-851f-114256292e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature matrix\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e1e09-805f-45a0-90db-4eaef1e9f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show feature names\n",
    "# count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5304fd3-22bb-41bd-8084-ed9e4ec9c5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4f4ad-46d0-4f21-842f-bd72396aae02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3208688-c3c1-4457-b5e3-503a42e380e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c184b2-342c-48ee-8c7f-b712f92c41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_le = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4156aef-76fe-4f29-a2a7-f3a87d52fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76964b-696c-4977-87dd-01f195521f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362e680-45e7-4675-ab81-db5864398aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29696b83-99cc-4455-91c7-5eb973ee2b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fa13b-3cd5-4310-99ab-a4990815be7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b31724d0-9910-4dde-b70d-12859afb6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfb8d64c-ea1e-4b66-9197-ffaee4369176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b272e345-7193-4982-930d-5036e748af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_le = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33225802-0edc-4d16-ab02-889015379de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31db50-521e-4d16-9e93-9dffc271389c",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4385e208-5e1a-4f4f-acd2-c6260f888d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and labels\n",
    "X = bag_of_words #text_data\n",
    "y = target_le #balancedDF.cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a2d68-e3b4-4a5d-93f0-2f7f5a207ae1",
   "metadata": {},
   "source": [
    "Still getting: Cast string to float is not supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2e12d-2dda-483e-bb18-de35a86d2ea0",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6977268e-759a-44b5-b850-fa720aad5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd987987-1799-490f-b0c3-27208343975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) # Default is 1/4 --> test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e973da6-4ab2-4df5-8f82-fc619b0e5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "805dcac1-53b5-4d11-95c7-1e44c8bcb5c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-0141de7e31d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36msequences_to_matrix\u001b[1;34m(self, sequences, mode)\u001b[0m\n\u001b[0;32m    411\u001b[0m                              'before using tfidf mode.')\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[0;32m    292\u001b[0m                         \" or shape[0]\")\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "features_train = tokenizer.sequences_to_matrix(X_train, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a031260-678b-4b69-bddb-69522726048e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<56250x41400 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 712244 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9cb16280-d067-4a64-975f-f2f2e7337961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=5000, split=',', char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20c67b72-b8fd-497d-9c37-60a41471adf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "      <th>txt_stems</th>\n",
       "      <th>txt_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[barely, better, gabbert, significantly, bette...</td>\n",
       "      <td>[bare, better, gabbert, significantli, better,...</td>\n",
       "      <td>bare better gabbert significantli better year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[fuck, ducks, angels, welcome, new, niners, fans]</td>\n",
       "      <td>[fuck, duck, angel, welcom, new, niner, fan]</td>\n",
       "      <td>fuck duck angel welcom new niner fan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt  \\\n",
       "0  sports  [barely, better, gabbert, significantly, bette...   \n",
       "1  sports  [fuck, ducks, angels, welcome, new, niners, fans]   \n",
       "\n",
       "                                           txt_stems  \\\n",
       "0  [bare, better, gabbert, significantli, better,...   \n",
       "1       [fuck, duck, angel, welcom, new, niner, fan]   \n",
       "\n",
       "                                             txt_str  \n",
       "0  bare better gabbert significantli better year ...  \n",
       "1               fuck duck angel welcom new niner fan  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = df.copy()\n",
    "testDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f43db3d-13ea-4320-969d-c5221987a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(testDF.txt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1150ac7-5fca-47e5-a02f-8f02ae9bc417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 664 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = tokenizer.texts_to_matrix(balancedDF.txt_str, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "233ab7a5-5845-4eec-9183-efab68aed724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 5000)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ddd0e3db-4203-4c17-beaa-0d308103d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = testDF.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42368aa4-ee4d-4139-b05e-695deba2e761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9985bda-1148-46bd-b9f6-cfbacc527a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode target vector to create a target matrix\n",
    "targets = to_categorical(target_le, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7259a3ad-efe9-40d4-bca5-43c9adbd0c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7f3f175f-1669-4341-82f3-52ff25c2eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = to_categorical([0,1,2,3], num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e11d17b0-d8b1-4481-84d8-fca230d76be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05a914-d394-48db-983a-45fda8b9b3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca5649-fa97-47b7-bef2-30e266fbfb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da780c78-c3df-4cdd-bca8-1774a8ac3d7b",
   "metadata": {},
   "source": [
    "## 2. Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efff7e18-9260-46a0-baf5-a7b1d1204a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing the Keras libraries and packages\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ed70190-d70a-40f3-97be-8de7ab995ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the keras model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(12, activation='relu')) # , input_dim=X_train.shape[1]\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee535b6d-c3a1-4f0f-9ca6-2b248d623205",
   "metadata": {},
   "source": [
    "## 3. Compile Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45b2dcae-8448-4617-909f-d5622b87a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75b252ab-bb04-4755-af24-72854087ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compile the keras model\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #  optimizer=RMSprop(lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0184e40-9dd5-405f-bac6-0fa4173a0868",
   "metadata": {},
   "source": [
    "## 4. Fit Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ffdf0250-4fcf-4e67-935e-811eecad6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # fit the keras model on the dataset\n",
    "# model.fit(features, targets, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6041520-d1b9-46c2-b70f-6bec292aa010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # fit the keras model on the dataset\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b3b738-1ce8-4101-8135-064a103e26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = balancedDF.txt_str\n",
    "y = balancedDF.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f6ebda9-f78f-4c85-8d97-27bbcc8b8d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_vector_train, target_vector_test = train_test_split(X, y) # Default is 1/4 --> test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c92f7bc-c35f-46aa-a964-eb4d0a9798a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b554e33e-f665-4178-8f83-295f5a8713db",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9430163-95ad-4c45-a828-be38dcaa6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# Convert feature data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features, split=',', char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "254e4f0e-4d30-44a9-a459-ee88c8e38217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 823 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d13321a-bf9c-4b92-970d-be5595fef1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = tokenizer.texts_to_matrix(data_train, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5170790-2567-4a83-8421-df882dacff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = tokenizer.texts_to_matrix(data_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6654bbd-8686-41a0-84b8-1da276227e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1acdd00a-6648-4ecf-9da7-dc1c7a480d55",
   "metadata": {},
   "source": [
    "Maybe skip this and use LabelBinarizer instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e7691-e42d-4234-91f2-cdc931a5f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb674bef-64eb-47a0-8989-9de30ec2479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98084788-1324-4eb5-800c-e3fb444c24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_train_le = le.transform(target_vector_train)\n",
    "# target_test_le = le.transform(target_vector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ca982-a95e-4cd8-8e35-e0921e969250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils.np_utils import to_categorical\n",
    "# # One-hot encode target vector to create a target matrix\n",
    "# target_train = to_categorical(target_train_le, num_classes=3)\n",
    "# target_test = to_categorical(target_test_le, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf894897-7aea-4c4f-b550-49d6dd601954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9130413-caaa-42a9-aa72-a5a8c2c63215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6883de12-94d8-4fc1-9de8-601922280c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "target_train = lb.transform(target_vector_train)\n",
    "target_test = lb.transform(target_vector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3905e-85e7-4b33-aef0-a82a0cf37e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6ee3b3b-caf0-476f-987a-84e9d0029653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "# Start neural network\n",
    "network = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92abb553-7722-4e8d-94e8-b5a9a79394a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=500,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=150, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(layers.Dense(units=3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1e7bf2e-7ac3-4e55-9f13-54a0a2410841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "595bb5d6-ed6e-422b-b8ed-515661b2972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 26s 33ms/step - loss: 1.0529 - accuracy: 0.3922 - val_loss: 1.0566 - val_accuracy: 0.3903\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 17s 22ms/step - loss: 1.0364 - accuracy: 0.4008 - val_loss: 1.0580 - val_accuracy: 0.3938\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 17s 22ms/step - loss: 1.0296 - accuracy: 0.4048 - val_loss: 1.0723 - val_accuracy: 0.3930\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 16s 22ms/step - loss: 1.0270 - accuracy: 0.4050 - val_loss: 1.0856 - val_accuracy: 0.3914\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 17s 22ms/step - loss: 1.0250 - accuracy: 0.4068 - val_loss: 1.0980 - val_accuracy: 0.3855\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target\n",
    "                      epochs=5, # Three epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=75, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8d32b-a310-40a8-b11e-98fe7994c83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbd0f1-8004-41c0-8e94-0ad68f92196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "# Create function returning a compiled network\n",
    "def create_network(optimizer='rmsprop'):\n",
    "    # Start neural network\n",
    "    network = Sequential()\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=500,\n",
    "                             activation=\"relu\",\n",
    "                             input_shape=(number_of_features,)))\n",
    "\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units=150, activation=\"relu\"))\n",
    "\n",
    "    # Add fully connected layer with a softmax activation function\n",
    "    network.add(layers.Dense(units=3, activation=\"softmax\"))\n",
    "    \n",
    "    # Compile neural network\n",
    "    network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                    optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                    metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "    \n",
    "    # Return compiled network\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e3ea4-1ac4-4328-a03e-bf740f88fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1c18d-6e6f-4b1a-98ba-6969e5c41895",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "neural_network = KerasClassifier(build_fn=create_network, verbose=1)\n",
    "\n",
    "# Create hyperparameter space\n",
    "epochs = [5, 15, 50]\n",
    "batches = [5, 25, 100]\n",
    "optimizers = [\"rmsprop\", \"adam\"]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n",
    "\n",
    "# Create grid search\n",
    "grid = GridSearchCV(estimator=neural_network, param_grid=hyperparameters)\n",
    "\n",
    "# Fit grid search\n",
    "# grid_result = grid.fit(features, target)\n",
    "grid_result = grid.fit(features_train, # Features\n",
    "                      target_train, # Target\n",
    "#                       epochs=150, # Three epochs\n",
    "                      verbose=1, # Some output\n",
    "#                       batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6e2e5-4c53-4d0b-ba31-d6009fb6ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View hyperparameters of best neural network\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb935e-a818-48a2-8e81-10c00d90678a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f81a3-1424-48aa-8623-cd51f37996f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from win32com.client import Dispatch\n",
    "speak = Dispatch(\"SAPI.SpVoice\").Speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f08a8-990b-4b2f-a0e8-595d15cbb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak(\"modeling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf007e8-4855-4c77-8d57-c02db7dbc1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
