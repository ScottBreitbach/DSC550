{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scott Breitbach\n",
    "## 10-May-2021\n",
    "## DSC550, Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3 Exercise: Neural Network Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Neural Network Classifier with Scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data into a list of dictionaries\n",
    "data = []\n",
    "with jsonlines.open('categorized-comments.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to DataFrame\n",
    "cat_comments_df = pd.DataFrame(data)\n",
    "cat_comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import sys\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data set to manipulate\n",
    "df = cat_comments_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of punctuation\n",
    "punctuation_dict = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "# Create a dictionary of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "def cleanText(string):\n",
    "    '''Processes string and returns cleaned up list of words'''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    string = string.translate(punctuation_dict)\n",
    "    \n",
    "    # Remove newlines\n",
    "    string = string.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Remove stopwords\n",
    "    string = [word for word in string.split() if word not in stopwords_dict]\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the text in the 'txt' column\n",
    "df.txt = df.txt.apply(lambda string: cleanText(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "df['txt_stems'] = df.txt.apply(lambda words: [porter.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Join tokenized stem words into a string\n",
    "df['txt_str'] = df.txt_stems.apply(lambda s: ' '.join(map(str, s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "      <th>txt_stems</th>\n",
       "      <th>txt_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>[barely, better, gabbert, significantly, bette...</td>\n",
       "      <td>[bare, better, gabbert, significantli, better,...</td>\n",
       "      <td>bare better gabbert significantli better year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>[fuck, ducks, angels, welcome, new, niners, fans]</td>\n",
       "      <td>[fuck, duck, angel, welcom, new, niner, fan]</td>\n",
       "      <td>fuck duck angel welcom new niner fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>[drafted, wrs, matt, millen, probably]</td>\n",
       "      <td>[draft, wr, matt, millen, probabl]</td>\n",
       "      <td>draft wr matt millen probabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[done]</td>\n",
       "      <td>[done]</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>[noo]</td>\n",
       "      <td>[noo]</td>\n",
       "      <td>noo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt  \\\n",
       "0  sports  [barely, better, gabbert, significantly, bette...   \n",
       "1  sports  [fuck, ducks, angels, welcome, new, niners, fans]   \n",
       "2  sports             [drafted, wrs, matt, millen, probably]   \n",
       "3  sports                                             [done]   \n",
       "4  sports                                              [noo]   \n",
       "\n",
       "                                           txt_stems  \\\n",
       "0  [bare, better, gabbert, significantli, better,...   \n",
       "1       [fuck, duck, angel, welcom, new, niner, fan]   \n",
       "2                 [draft, wr, matt, millen, probabl]   \n",
       "3                                             [done]   \n",
       "4                                              [noo]   \n",
       "\n",
       "                                             txt_str  \n",
       "0  bare better gabbert significantli better year ...  \n",
       "1               fuck duck angel welcom new niner fan  \n",
       "2                       draft wr matt millen probabl  \n",
       "3                                               done  \n",
       "4                                                noo  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Set Into Equal-Sized Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by category\n",
    "cat_group = df.groupby('cat', as_index=False, group_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 25000 rows from each category\n",
    "balancedDF = cat_group.apply(lambda s: s.sample(25000, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_games               25000\n",
       "sports                    25000\n",
       "science_and_technology    25000\n",
       "Name: cat, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify counts of categories\n",
    "balancedDF.cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Text for Model-Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Feature Data to a Word-Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokenized lists of words into a list word strings\n",
    "text_data, string = [], \" \"\n",
    "\n",
    "for text in balancedDF.txt_stems:\n",
    "    text_data.append(string.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<75000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 874543 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word-count vector as a sparse matrix\n",
    "count = CountVectorizer(max_features=5000)\n",
    "bal_sparseWCV = count.fit_transform(text_data)\n",
    "bal_sparseWCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and labels\n",
    "X = bal_sparseWCV\n",
    "y = balancedDF.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After too many tries with `GridSearchCV` taking several hours and/or freezing up overnight I tried `RandomizedSearchPV` and had much better luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Iteration 1, loss = 0.73768681\n",
      "Validation score: 0.740622\n",
      "Iteration 2, loss = 0.54187629\n",
      "Validation score: 0.741867\n",
      "Iteration 3, loss = 0.49838411\n",
      "Validation score: 0.744889\n",
      "Iteration 4, loss = 0.47766933\n",
      "Validation score: 0.746667\n",
      "Iteration 5, loss = 0.46601224\n",
      "Validation score: 0.745600\n",
      "Iteration 6, loss = 0.45828540\n",
      "Validation score: 0.739378\n",
      "Iteration 7, loss = 0.45208243\n",
      "Validation score: 0.740622\n",
      "Iteration 8, loss = 0.44857375\n",
      "Validation score: 0.739378\n",
      "Iteration 9, loss = 0.44477580\n",
      "Validation score: 0.738133\n",
      "Iteration 10, loss = 0.44193440\n",
      "Validation score: 0.738133\n",
      "Iteration 11, loss = 0.43963454\n",
      "Validation score: 0.739911\n",
      "Iteration 12, loss = 0.43758229\n",
      "Validation score: 0.735822\n",
      "Iteration 13, loss = 0.43580811\n",
      "Validation score: 0.736000\n",
      "Iteration 14, loss = 0.43470887\n",
      "Validation score: 0.738844\n",
      "Iteration 15, loss = 0.43317936\n",
      "Validation score: 0.736000\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Wall time: 1h 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlp_gs = MLPClassifier(max_iter=100, verbose=True)#, early_stopping=True) max_iter=200, 300\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(30,), (100,)], #[(10, 10, 10), (20,20, 20), (40,), (50, 30), (500, 150)],\n",
    "    'activation': ['relu', 'tanh'], \n",
    "    'solver': ['adam', 'sgd'], \n",
    "    'alpha': uniform(loc=0, scale=0.05),\n",
    "    'learning_rate': ['constant', 'adaptive'], # only used when solver is 'sgd'\n",
    "    'early_stopping': [True],\n",
    "}\n",
    "rand = RandomizedSearchCV(mlp_gs, parameter_space, random_state=42, n_iter=10, verbose=2, n_jobs=-1, cv=5)\n",
    "rand_result = rand.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'tanh', 'alpha': 0.0011531212520707879, 'early_stopping': True, 'hidden_layer_sizes': (30,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', rand_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the test set:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.81      0.78      0.80      6332\n",
      "                sports       0.72      0.76      0.74      6236\n",
      "           video_games       0.70      0.69      0.70      6182\n",
      "\n",
      "              accuracy                           0.74     18750\n",
      "             macro avg       0.74      0.74      0.74     18750\n",
      "          weighted avg       0.75      0.74      0.74     18750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test , rand_result.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "bal_mlp = MLPClassifier(max_iter=500, hidden_layer_sizes=(30,), activation='tanh', \n",
    "                        solver='adam', alpha=0.001, learning_rate='constant', early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.001, early_stopping=True,\n",
       "              hidden_layer_sizes=(30,), max_iter=500)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the training data to the model\n",
    "bal_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = bal_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5008  543  781]\n",
      " [ 478 4687 1071]\n",
      " [ 670 1167 4345]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision / Recall / F1 / Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.81      0.79      0.80      6332\n",
      "                sports       0.73      0.75      0.74      6236\n",
      "           video_games       0.70      0.70      0.70      6182\n",
      "\n",
      "              accuracy                           0.75     18750\n",
      "             macro avg       0.75      0.75      0.75     18750\n",
      "          weighted avg       0.75      0.75      0.75     18750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, f1-score, and accuracy\n",
    "cr = classification_report(y_test, predictions)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
