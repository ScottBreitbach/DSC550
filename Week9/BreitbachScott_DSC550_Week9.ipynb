{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scott Breitbach\n",
    "## 10-May-2021\n",
    "## DSC550, Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3 Exercise: Neural Network Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Neural Network Classifier with Scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data into a list of dictionaries\n",
    "data = []\n",
    "with jsonlines.open('categorized-comments.jsonl') as reader:\n",
    "    for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "        data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sports</td>\n",
       "      <td>Fuck the ducks and the Angels! But welcome to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should have drafted more WRs.\\n\\n- Matt Millen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sports</td>\n",
       "      <td>[Done](https://i.imgur.com/2YZ90pm.jpg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>No!! NOO!!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant...\n",
       "1  sports  Fuck the ducks and the Angels! But welcome to ...\n",
       "2  sports  Should have drafted more WRs.\\n\\n- Matt Millen...\n",
       "3  sports            [Done](https://i.imgur.com/2YZ90pm.jpg)\n",
       "4  sports                                      No!! NOO!!!!!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to DataFrame\n",
    "cat_comments_df = pd.DataFrame(data)\n",
    "cat_comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import sys\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data set to manipulate\n",
    "df = cat_comments_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of punctuation\n",
    "punctuation_dict = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                            if unicodedata.category(chr(i)).startswith('P'))\n",
    "# Create a dictionary of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "def cleanText(string):\n",
    "    '''Processes string and returns cleaned up list of words'''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    string = string.translate(punctuation_dict)\n",
    "    \n",
    "    # Remove newlines\n",
    "    string = string.replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Remove stopwords\n",
    "    string = [word for word in string.split() if word not in stopwords_dict]\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the text in the 'txt' column\n",
    "df.txt = df.txt.apply(lambda string: cleanText(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Apply PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "df['txt_stems'] = df.txt.apply(lambda words: [porter.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Join tokenized stem words into a string\n",
    "df['txt_str'] = df.txt_stems.apply(lambda s: ' '.join(map(str, s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Set Into Equal-Sized Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by category\n",
    "cat_group = df.groupby('cat', as_index=False, group_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 25000 rows from each category\n",
    "balancedDF = cat_group.apply(lambda s: s.sample(25000, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify counts of categories\n",
    "balancedDF.cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Text for Model-Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Feature Data to a Word-Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokenized lists of words into a list word strings\n",
    "text_data, string = [], \" \"\n",
    "\n",
    "for text in balancedDF.txt_stems:\n",
    "    text_data.append(string.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-count vector as a sparse matrix\n",
    "count = CountVectorizer(max_features=5000)\n",
    "bal_sparseWCV = count.fit_transform(text_data)\n",
    "bal_sparseWCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and labels\n",
    "X = bal_sparseWCV\n",
    "y = balancedDF.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mlp_gs = MLPClassifier(max_iter=100, verbose=True)#, early_stopping=True) max_iter=200, 300\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(30,), (100,)], #[(10, 10, 10), (20,20, 20), (40,), (50, 30), (500, 150)],\n",
    "    'activation': ['relu', 'tanh'], \n",
    "    'solver': ['adam', 'sgd'], \n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant', 'adaptive'], # only used when solver is 'sgd'\n",
    "}\n",
    "grid = GridSearchCV(mlp_gs, parameter_space, verbose=2, n_jobs=-1, cv=5)\n",
    "grid_result = grid.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters found:\\n', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test , grid_result.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.0005, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the test set:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.74      0.78      0.76      6275\n",
      "                sports       0.70      0.72      0.71      6223\n",
      "           video_games       0.69      0.63      0.66      6252\n",
      "\n",
      "              accuracy                           0.71     18750\n",
      "             macro avg       0.71      0.71      0.71     18750\n",
      "          weighted avg       0.71      0.71      0.71     18750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test , grid_result.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No.1, using `early_stopping=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.0005, 'hidden_layer_sizes': (30,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the test set:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "science_and_technology       0.84      0.81      0.82      6275\n",
      "                sports       0.74      0.81      0.77      6223\n",
      "           video_games       0.76      0.71      0.74      6252\n",
      "\n",
      "              accuracy                           0.78     18750\n",
      "             macro avg       0.78      0.78      0.78     18750\n",
      "          weighted avg       0.78      0.78      0.78     18750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test , grid_result.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from win32com.client import Dispatch\n",
    "speak = Dispatch(\"SAPI.SpVoice\").Speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak(\"modeling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
